{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: tensor([-3.1032, -0.2769,  2.2583,  1.0565,  0.7202,  0.1029, -1.9237, -0.5518,\n",
      "        -1.7343, -0.4385, -0.4145, -2.4887,  0.3950, -0.1805,  1.4258, -1.2152,\n",
      "         0.0367, -1.6281,  0.4144,  0.0994, -0.9635,  1.0568,  0.2574, -0.1623,\n",
      "        -1.3715, -0.1412,  0.5866,  0.1228, -0.2036, -0.3358,  0.5903, -0.4167])\n",
      "k: tensor([ 1.6162e+00,  6.8729e-01, -1.1949e+00,  1.8411e+00, -4.9815e-01,\n",
      "         6.7064e-01,  8.6041e-01,  4.7778e-01,  9.4828e-01, -1.2916e+00,\n",
      "         1.6296e-01,  1.0296e+00,  7.9215e-01,  4.7766e-01,  6.1852e-01,\n",
      "        -2.1944e-01, -3.2050e-01, -5.8963e-01, -1.5785e-03,  3.2309e-01,\n",
      "        -1.0185e+00,  7.7100e-01,  1.6684e+00, -1.5183e-01, -8.4329e-01,\n",
      "         7.7285e-01,  2.8210e-01, -5.9170e-02,  9.7809e-01, -5.1536e-02,\n",
      "         4.2023e-01, -2.3006e-01])\n",
      "q_new:  tensor([-3.1032, -0.2769,  2.2583,  1.0565,  0.7202,  0.1029, -1.9237, -0.5518,\n",
      "        -1.7343, -0.4385, -0.4145, -2.4887,  0.3950, -0.1805,  1.4258, -1.2152,\n",
      "         0.0367, -1.6281,  0.4144,  0.0994, -0.9635,  1.0568,  0.2574, -0.1623,\n",
      "        -1.3715, -0.1412,  0.5866,  0.1228, -0.2036, -0.3358,  0.5903, -0.4167])\n",
      "k_new:  tensor([ 1.6162e+00,  6.8729e-01, -1.1949e+00,  1.8411e+00, -4.9815e-01,\n",
      "         6.7064e-01,  8.6041e-01,  4.7778e-01,  9.4828e-01, -1.2916e+00,\n",
      "         1.6296e-01,  1.0296e+00,  7.9215e-01,  4.7766e-01,  6.1852e-01,\n",
      "        -2.1944e-01, -3.2050e-01, -5.8963e-01, -1.5785e-03,  3.2309e-01,\n",
      "        -1.0185e+00,  7.7100e-01,  1.6684e+00, -1.5183e-01, -8.4329e-01,\n",
      "         7.7285e-01,  2.8210e-01, -5.9170e-02,  9.7809e-01, -5.1536e-02,\n",
      "         4.2023e-01, -2.3006e-01])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from torch import einsum, nn\n",
    "\n",
    "def find_correction_factor(num_rotations, dim, base=10000, max_position_embeddings=2048):\n",
    "    # Inverse dim formula to find number of rotations\n",
    "    return (dim * math.log(max_position_embeddings/(num_rotations * 2 * math.pi)))/(2 * math.log(base))\n",
    "\n",
    "\n",
    "def find_correction_range(low_rot, high_rot, dim, base=10000, max_position_embeddings=2048):\n",
    "    low = math.floor(find_correction_factor(\n",
    "        low_rot, dim, base, max_position_embeddings))\n",
    "    high = math.ceil(find_correction_factor(\n",
    "        high_rot, dim, base, max_position_embeddings))\n",
    "    return max(low, 0), min(high, dim-1)  # Clamp values just in case\n",
    "\n",
    "\n",
    "def linear_ramp_mask(min, max, dim):\n",
    "    if min == max:\n",
    "        max += 0.001  # Prevent singularity\n",
    "\n",
    "    linear_func = (torch.arange(dim, dtype=torch.float32) - min) / (max - min)\n",
    "    ramp_func = torch.clamp(linear_func, 0, 1)\n",
    "    return ramp_func\n",
    "\n",
    "\n",
    "def find_newbase_ntk(dim, base=10000, scale=1):\n",
    "    return base * scale ** (dim / (dim-2))\n",
    "\n",
    "\n",
    "class DynamicPartNTKScaledRotaryEmbedding(torch.nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=5, original_max_position_embeddings=5, base=10000, ntk_factor=1, extrapolation_factor=1, finetuned=False, device=None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.base = base\n",
    "        self.ntk_factor = ntk_factor\n",
    "        self.extrapolation_factor = extrapolation_factor\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        if finetuned:\n",
    "            self.ntk(self.max_position_embeddings / original_max_position_embeddings, device)\n",
    "        else:\n",
    "            inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))\n",
    "            self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "        # Build here to make `torch.jit.trace` work.\n",
    "        self.max_seq_len_cached = max_position_embeddings\n",
    "        t = torch.arange(self.max_seq_len_cached,\n",
    "                         device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        dtype = torch.get_default_dtype()\n",
    "        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n",
    "\n",
    "    def forward(self, x, seq_len=None):\n",
    "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
    "        # This `if` block is unlikely to be run after we build sin/cos in `__init__`. Keep the logic here just in case.\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self.max_seq_len_cached = seq_len\n",
    "\n",
    "            self.ntk(seq_len / self.max_position_embeddings, x.device)\n",
    "\n",
    "            t = torch.arange(self.max_seq_len_cached,\n",
    "                             device=x.device, dtype=self.inv_freq.dtype)\n",
    "            freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "            # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n",
    "            self.register_buffer(\"cos_cached\", emb.cos()[\n",
    "                                 None, None, :, :].to(x.dtype), persistent=False)\n",
    "            self.register_buffer(\"sin_cached\", emb.sin()[\n",
    "                                 None, None, :, :].to(x.dtype), persistent=False)\n",
    "        return (\n",
    "            self.cos_cached[:seq_len].to(dtype=x.dtype),\n",
    "            self.sin_cached[:seq_len].to(dtype=x.dtype),\n",
    "        )\n",
    "    \n",
    "    def ntk(self, scale, device):\n",
    "\n",
    "        # Interpolation constants found experimentally for LLaMA (might not be totally optimal though)\n",
    "        # Do not change unless there is a good reason for doing so!\n",
    "        beta_0 = 1.25\n",
    "        beta_1 = 0.75\n",
    "        gamma_0 = 16\n",
    "        gamma_1 = 2\n",
    "\n",
    "        # Three RoPE extrapolation/interpolation methods\n",
    "        inv_freq_base = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n",
    "        inv_freq_linear = 1.0 / (scale * (self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim)))\n",
    "        inv_freq_ntk = 1.0 / (find_newbase_ntk(self.dim, self.base, scale)\n",
    "                              ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n",
    "\n",
    "        current_dtype = inv_freq_ntk.dtype\n",
    "        current_device = inv_freq_ntk.device\n",
    "\n",
    "        # Combine NTK and Linear\n",
    "        low, high = find_correction_range(\n",
    "            beta_0, beta_1, self.dim, self.base, self.max_position_embeddings)\n",
    "        inv_freq_mask = (1 - linear_ramp_mask(low, high, self.dim // 2).type(current_dtype).to(current_device)) * self.ntk_factor\n",
    "        inv_freq = inv_freq_linear * (1 - inv_freq_mask) + inv_freq_ntk * inv_freq_mask\n",
    "\n",
    "        # Combine Extrapolation and NTK and Linear\n",
    "        low, high = find_correction_range(\n",
    "            gamma_0, gamma_1, self.dim, self.base, self.max_position_embeddings)\n",
    "        inv_freq_mask = (1 - linear_ramp_mask(low, high, self.dim // 2).type(current_dtype).to(current_device)) * self.extrapolation_factor\n",
    "        inv_freq = inv_freq * (1 - inv_freq_mask) + inv_freq_base * inv_freq_mask\n",
    "\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n",
    "    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "\n",
    "\n",
    "# (bs, head, length, dim)\n",
    "q = torch.randn((2, 12, 10, 32))  # q=[q0, q1, .., qd-1]\n",
    "k = torch.randn((2, 12, 10, 32))\n",
    "v = torch.randn((2, 12, 10, 32))\n",
    "position_ids = torch.tensor(list(range(0, 10))).unsqueeze(0).repeat(2, 1)\n",
    "print('q:', q[0][0][0])\n",
    "print('k:', k[0][0][0])\n",
    "rotary_emb = DynamicPartNTKScaledRotaryEmbedding(dim=32,max_position_embeddings=10)\n",
    "cos, sin = rotary_emb(v, seq_len=10)\n",
    "q_new, k_new = apply_rotary_pos_emb(q, k, cos, sin, position_ids)\n",
    "print('q_new: ', q_new[0][0][0])\n",
    "print('k_new: ', k_new[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: tensor([ 0.4122, -0.9974, -0.4258,  0.8649,  0.2575,  0.9717, -1.8156, -0.1367,\n",
      "         1.6512, -1.3428,  0.2563,  1.1431,  0.3470,  0.1834,  0.7185, -0.8874,\n",
      "         0.5366,  0.4353, -0.9806, -0.9068,  1.0905,  0.1720,  0.8175,  1.0371,\n",
      "        -0.1976,  1.3093, -0.9622, -0.7057, -0.8805, -1.0351, -0.5688, -0.2100])\n",
      "k: tensor([-0.1960, -0.1691,  0.5125,  0.7086, -0.2739,  0.5460,  0.1985, -0.6967,\n",
      "        -0.6941, -0.2064,  1.7953, -0.2109,  2.0027,  0.8383,  1.4107, -1.8471,\n",
      "         1.2018, -0.2565,  1.5268, -1.7817,  0.9530,  0.5201,  0.3590,  0.7296,\n",
      "        -0.0674,  0.4554,  0.4298,  0.7931,  0.5609, -0.2021,  0.0628,  0.2346])\n",
      "q_new:  tensor([ 0.4122, -0.9974, -0.4258,  0.8649,  0.2575,  0.9717, -1.8156, -0.1367,\n",
      "         1.6512, -1.3428,  0.2563,  1.1431,  0.3470,  0.1834,  0.7185, -0.8874,\n",
      "         0.5366,  0.4353, -0.9806, -0.9068,  1.0905,  0.1720,  0.8175,  1.0371,\n",
      "        -0.1976,  1.3093, -0.9622, -0.7057, -0.8805, -1.0351, -0.5688, -0.2100])\n",
      "k_new:  tensor([-0.1960, -0.1691,  0.5125,  0.7086, -0.2739,  0.5460,  0.1985, -0.6967,\n",
      "        -0.6941, -0.2064,  1.7953, -0.2109,  2.0027,  0.8383,  1.4107, -1.8471,\n",
      "         1.2018, -0.2565,  1.5268, -1.7817,  0.9530,  0.5201,  0.3590,  0.7296,\n",
      "        -0.0674,  0.4554,  0.4298,  0.7931,  0.5609, -0.2021,  0.0628,  0.2346])\n"
     ]
    }
   ],
   "source": [
    "def get_mscale(scale=1):\n",
    "    if scale <= 1:\n",
    "        return 1.0\n",
    "    return 0.1 * math.log(scale) + 1.0\n",
    "\n",
    "\n",
    "class YaRNScaledRotaryEmbedding(torch.nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=5, base=10000, scale=1, original_max_position_embeddings=2048, extrapolation_factor=1, attn_factor=1, beta_fast=32, beta_slow=1, finetuned=False, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        self.scale = scale\n",
    "        self.original_max_position_embeddings = original_max_position_embeddings\n",
    "        self.extrapolation_factor = extrapolation_factor\n",
    "        self.attn_factor = attn_factor\n",
    "        self.beta_fast = beta_fast\n",
    "        self.beta_slow = beta_slow\n",
    "\n",
    "        self.yarn(device)\n",
    "\n",
    "        # Build here to make `torch.jit.trace` work.\n",
    "        self.max_seq_len_cached = max_position_embeddings\n",
    "        t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        dtype = torch.get_default_dtype()\n",
    "\n",
    "        self.register_buffer(\"cos_cached\", (emb.cos() * self.mscale).to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", (emb.sin() * self.mscale).to(dtype), persistent=False)\n",
    "\n",
    "    def forward(self, x, seq_len=None):\n",
    "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
    "        # This `if` block is unlikely to be run after we build sin/cos in `__init__`. Keep the logic here just in case.\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self.max_seq_len_cached = seq_len\n",
    "\n",
    "            t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)\n",
    "            freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "            # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n",
    "\n",
    "            self.register_buffer(\"cos_cached\", (emb.cos() * self.mscale).to(x.dtype), persistent=False)\n",
    "            self.register_buffer(\"sin_cached\", (emb.sin() * self.mscale).to(x.dtype), persistent=False)\n",
    "        return (\n",
    "            self.cos_cached[:seq_len].to(dtype=x.dtype),\n",
    "            self.sin_cached[:seq_len].to(dtype=x.dtype),\n",
    "        )\n",
    "\n",
    "    def yarn(self, device):\n",
    "        pos_freqs = self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim)\n",
    "        inv_freq_extrapolation = 1.0 / pos_freqs\n",
    "        inv_freq_interpolation = 1.0 / (self.scale * pos_freqs)\n",
    "\n",
    "        low, high = find_correction_range(self.beta_fast, self.beta_slow, self.dim, self.base, self.original_max_position_embeddings)\n",
    "        inv_freq_mask = (1 - linear_ramp_mask(low, high, self.dim // 2).float().to(device)) * self.extrapolation_factor # Get n-d rotational scaling corrected for extrapolation\n",
    "        inv_freq = inv_freq_interpolation * (1 - inv_freq_mask) + inv_freq_extrapolation * inv_freq_mask\n",
    "\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self.mscale = float(get_mscale(self.scale) * self.attn_factor) # Get n-d magnitude scaling corrected for interpolation\n",
    "\n",
    "\n",
    "# (bs, head, length, dim)\n",
    "q = torch.randn((2, 12, 10, 32))  # q=[q0, q1, .., qd-1]\n",
    "k = torch.randn((2, 12, 10, 32))\n",
    "v = torch.randn((2, 12, 10, 32))\n",
    "position_ids = torch.tensor(list(range(0, 10))).unsqueeze(0).repeat(2, 1)\n",
    "print('q:', q[0][0][0])\n",
    "print('k:', k[0][0][0])\n",
    "rotary_emb = YaRNScaledRotaryEmbedding(dim=32,max_position_embeddings=10)\n",
    "cos, sin = rotary_emb(v, seq_len=10)\n",
    "q_new, k_new = apply_rotary_pos_emb(q, k, cos, sin, position_ids)\n",
    "print('q_new: ', q_new[0][0][0])\n",
    "print('k_new: ', k_new[0][0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
