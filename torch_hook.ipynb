{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 PyTorch 中，hook 是一种用于在神经网络的前向传播或反向传播过程中插入自定义操作的机制。hook 可以用于调试、可视化、梯度裁剪等任务。PyTorch 提供了三种类型的 hook：\n",
    "\n",
    "- Forward Hook：在前向传播过程中执行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside Linear forward hook\n",
      "Input: (tensor([[ 1.2408, -0.7267,  0.2505, -1.8252, -1.7672, -1.7213,  1.7390, -0.4912,\n",
      "          1.1297,  1.5345]]),)\n",
      "Output: tensor([[ 1.2112, -0.6202, -0.4395, -0.1143, -0.4221]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Modified Input: (tensor([[ 2.4816, -1.4534,  0.5010, -3.6504, -3.5343, -3.4426,  3.4780, -0.9825,\n",
      "          2.2594,  3.0689]]),)\n",
      "Modified Output: tensor([[121118.4531, -62017.8086, -43953.1016, -11430.0859, -42211.1094]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([[19626.5566]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义一个简单的神经网络\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "        self.fc2 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 创建网络实例\n",
    "net = SimpleNet()\n",
    "\n",
    "# 定义一个 forward hook 函数\n",
    "def forward_hook(module, input, output):\n",
    "    print(f\"Inside {module.__class__.__name__} forward hook\")\n",
    "    print(f\"Input: {input}\")\n",
    "    print(f\"Output: {output}\")\n",
    "\n",
    "# 定义一个 forward hook 函数\n",
    "def forward_hook2(module, input, output):\n",
    "    print(f\"Inside {module.__class__.__name__} forward hook\")\n",
    "    print(f\"Input: {input}\")\n",
    "    print(f\"Output: {output}\")\n",
    "    modified_input = (input[0] * 2,)\n",
    "    print(f\"Modified Input: {modified_input}\")\n",
    "\n",
    "    modified_output = output * 100000\n",
    "    print(f\"Modified Output: {modified_output}\")\n",
    "    return modified_output\n",
    "    \n",
    "\n",
    "# 注册 forward hook\n",
    "hook_handle = net.fc1.register_forward_hook(forward_hook2)\n",
    "# hook_handle2 = net.fc2.register_forward_hook(forward_hook2)\n",
    "\n",
    "# 创建一个随机输入\n",
    "x = torch.randn(1, 10)\n",
    "\n",
    "# 前向传播\n",
    "output = net(x)\n",
    "print(output)\n",
    "\n",
    "# 移除 hook\n",
    "# hook_handle.remove()\n",
    "hook_handle.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Backward Hook：在反向传播过程中执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside Linear backward hook\n",
      "Grad Input: (tensor([-0.0967, -0.3673,  0.0775,  0.1022, -0.0757]), tensor([[-0.0853,  0.0116, -0.0570, -0.0550, -0.0887, -0.0518,  0.0237,  0.0228,\n",
      "          0.0409,  0.0147]]), tensor([[-0.0214, -0.0812,  0.0171,  0.0226, -0.0167],\n",
      "        [ 0.0058,  0.0221, -0.0047, -0.0062,  0.0046],\n",
      "        [ 0.1036,  0.3937, -0.0831, -0.1095,  0.0812],\n",
      "        [ 0.0635,  0.2411, -0.0509, -0.0671,  0.0497],\n",
      "        [ 0.0156,  0.0591, -0.0125, -0.0164,  0.0122],\n",
      "        [ 0.0289,  0.1098, -0.0232, -0.0305,  0.0226],\n",
      "        [-0.0808, -0.3071,  0.0648,  0.0854, -0.0633],\n",
      "        [ 0.0338,  0.1285, -0.0271, -0.0357,  0.0265],\n",
      "        [-0.0303, -0.1153,  0.0243,  0.0321, -0.0238],\n",
      "        [ 0.1349,  0.5124, -0.1081, -0.1425,  0.1057]]))\n",
      "Grad Output: (tensor([[-0.0967, -0.3673,  0.0775,  0.1022, -0.0757]]),)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzd/miniconda3/envs/zzd-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义一个简单的神经网络\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "        self.fc2 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 创建网络实例\n",
    "net = SimpleNet()\n",
    "\n",
    "# 定义一个 backward hook 函数\n",
    "def backward_hook(module, grad_input, grad_output):\n",
    "    print(f\"Inside {module.__class__.__name__} backward hook\")\n",
    "    print(f\"Grad Input: {grad_input}\")\n",
    "    print(f\"Grad Output: {grad_output}\")\n",
    "\n",
    "# 注册 backward hook\n",
    "hook_handle = net.fc1.register_backward_hook(backward_hook)\n",
    "\n",
    "# 创建一个随机输入\n",
    "x = torch.randn(1, 10, requires_grad=True)\n",
    "\n",
    "# 前向传播\n",
    "output = net(x)\n",
    "\n",
    "# 计算损失\n",
    "loss = output.sum()\n",
    "\n",
    "# 反向传播\n",
    "loss.backward()\n",
    "\n",
    "# 移除 hook\n",
    "hook_handle.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pre-Forward Hook：在前向传播之前执行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside Linear pre-forward hook\n",
      "Input: (tensor([[-0., 0., -0., -0., 0., -0., 0., 0., -0., 0.]]),)\n",
      "Output: tensor([[0.]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义一个简单的神经网络\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 5, bias=False)\n",
    "        self.fc2 = nn.Linear(5, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 创建网络实例\n",
    "net = SimpleNet()\n",
    "\n",
    "# 定义一个 pre-forward hook 函数\n",
    "def pre_forward_hook(module, input):\n",
    "    print(f\"Inside {module.__class__.__name__} pre-forward hook\")\n",
    "    input = (input[0] * 0,)\n",
    "    print(f\"Input: {input}\")\n",
    "    return input\n",
    "\n",
    "# 注册 pre-forward hook\n",
    "hook_handle = net.fc1.register_forward_pre_hook(pre_forward_hook)\n",
    "\n",
    "# 创建一个随机输入\n",
    "x = torch.randn(1, 10)\n",
    "\n",
    "# 前向传播\n",
    "output = net(x)\n",
    "print(f\"Output: {output}\")\n",
    "\n",
    "# 移除 hook\n",
    "hook_handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[-0.1126]], grad_fn=<MmBackward0>)\n",
      "\n",
      "Inside Linear post-backward hook\n",
      "Original grad_input: [torch.Size([1, 10])]\n",
      "Original grad_output: [torch.Size([1, 5])]\n",
      "\n",
      "Gradients after backward:\n",
      "fc1.weight: torch.Size([5, 10])\n",
      "fc2.weight: torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 定义一个 post-backward hook 函数\n",
    "def post_backward_hook(module, grad_input, grad_output):\n",
    "    print(f\"\\nInside {module.__class__.__name__} post-backward hook\")\n",
    "    print(f\"Original grad_input shapes: {[g.shape if g is not None else None for g in grad_input]}\")\n",
    "    print(f\"Original grad_output shapes: {[g.shape if g is not None else None for g in grad_output]}\")\n",
    "    \n",
    "    # 修改梯度示例：将第一个梯度输入乘以0.5\n",
    "    modified_grad_input = list(grad_input)\n",
    "    if modified_grad_input[0] is not None:\n",
    "        modified_grad_input[0] = modified_grad_input[0] * 0.01\n",
    "    \n",
    "    return tuple(modified_grad_input)\n",
    "\n",
    "# 注册 post-backward hook\n",
    "hook_handle = net.fc1.register_full_backward_hook(post_backward_hook)\n",
    "\n",
    "# 创建一个随机输入和标签\n",
    "x = torch.randn(1, 10, requires_grad=True)\n",
    "target = torch.randn(1, 1)\n",
    "\n",
    "# 前向传播\n",
    "output = net(x)\n",
    "print(f\"Output: {output}\")\n",
    "\n",
    "# 定义损失函数\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(output, target)\n",
    "\n",
    "# 反向传播\n",
    "loss.backward()\n",
    "\n",
    "# 打印梯度\n",
    "print(\"\\nGradients after backward:\")\n",
    "for name, param in net.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"{name}: {param.grad.shape}\")\n",
    "\n",
    "# 移除 hook\n",
    "hook_handle.remove()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
